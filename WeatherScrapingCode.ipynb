{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist\n"
     ]
    }
   ],
   "source": [
    "#creating files\n",
    "try:\n",
    "    CSVfile = open('WeatherScraping.csv','x')\n",
    "    CSVfile.write(\"page,scraping_date,hour,temp,wind\"+\"\\n\")\n",
    "    CSVfile.close()\n",
    "    JSONfile = open('WeatherScraping.json','x')\n",
    "    JSONfile.close()\n",
    "    print (\"Created files\")\n",
    "except FileExistsError:\n",
    "    print(\"Files already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import statistics\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do You really want to clear all data? (Type 'YES' to continue)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-36898d8d7e7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#clearing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Do You really want to clear all data? (Type 'YES' to continue)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"YES\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mCSVfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'WeatherScraping.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mCSVfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"page,scraping_date,hour,temp,wind\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#clearing data\n",
    "print(\"Do You really want to clear all data? (Type 'YES' to continue)\")\n",
    "if str(input())==\"YES\":\n",
    "    CSVfile = open('WeatherScraping.csv','w')\n",
    "    CSVfile.write(\"page,scraping_date,hour,temp,wind\"+\"\\n\")\n",
    "    CSVfile.close()\n",
    "    JSONfile = open('WeatherScraping.json','w')\n",
    "    JSONfile.close()\n",
    "    print(\"Data cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from dobrapogoda24.pl scraped successfully\n"
     ]
    }
   ],
   "source": [
    "#Website dobrapogoda24.pl (1)\n",
    "\n",
    "#Load website HTML\n",
    "page=\"https://dobrapogoda24.pl/pogoda/gdansk\"\n",
    "r = requests.get(page)\n",
    "host = re.search('https://(.+?)/', page).group(1)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "#load values from HTML\n",
    "hours = soup.findAll('div',attrs = {'class':'cell time'})[:5]\n",
    "temps = soup.findAll('div',attrs = {'class':'cell temp'})[:5]\n",
    "\n",
    "#Write values to vectors\n",
    "v_hours=[]\n",
    "v_temps=[]\n",
    "for temp in temps:\n",
    "    v_temps.append(temp.text.strip())\n",
    "for hour in hours:\n",
    "    v_hours.append(hour.text.strip())\n",
    "    \n",
    "#Write data from vectors to files\n",
    "CSVfile = open('WeatherScraping.csv','a')\n",
    "JSONfile = open('WeatherScraping.json','a')\n",
    "for i in range(5):\n",
    "    v_temps[i]=v_temps[i][:-1]\n",
    "    CSVfile.write(\n",
    "        host+\",\"\n",
    "        +datetime.today().strftime('%Y-%m-%d')+\",\"\n",
    "        +v_hours[i]+\",\"\n",
    "        +v_temps[i]+\",\"\n",
    "        +\"NULL\"+\n",
    "        \"\\n\")\n",
    "    JSONfile.write(\"{'page':'\"+host+\n",
    "                   \"','scraping_date':'\"+datetime.today().strftime('%Y-%m-%d')+\n",
    "                   \"','hour':'\"+v_hours[i]+\n",
    "                   \"','temp':'\"+v_temps[i]+\n",
    "                   \"'}\\n\")  \n",
    "CSVfile.close()\n",
    "JSONfile.close()\n",
    "print(\"Data from \" + host + \" scraped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from pogoda.interia.pl scraped successfully\n"
     ]
    }
   ],
   "source": [
    "#Website pogoda.interia.pl (2)\n",
    "\n",
    "#Load website HTML\n",
    "page=\"https://pogoda.interia.pl/prognoza-szczegolowa-gdansk,cId,8048\"\n",
    "r = requests.get(page)\n",
    "host = re.search('https://(.+?)/', page).group(1)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "#load values from HTML\n",
    "hours = soup.findAll('span',attrs = {'class':'hour'})[:5]\n",
    "temps = soup.findAll('span',attrs = {'class':'forecast-temp'})[:5]\n",
    "\n",
    "#Write values to vectors\n",
    "v_hours=[]\n",
    "v_temps=[]\n",
    "for temp in temps:\n",
    "    v_temps.append(temp.text.strip('Â°C'))\n",
    "for hour in hours:\n",
    "    v_hours.append(hour.text.strip())\n",
    "    \n",
    "#Write data from vectors to files\n",
    "CSVfile = open('WeatherScraping.csv','a')\n",
    "JSONfile = open('WeatherScraping.json','a')\n",
    "for i in range(5):\n",
    "    CSVfile.write(\n",
    "        host+\",\"\n",
    "        +datetime.today().strftime('%Y-%m-%d')+\",\"\n",
    "        +v_hours[i]+\":00\"+\",\"\n",
    "        +v_temps[i]+\",\"\n",
    "        +\"NULL\"+\n",
    "        \"\\n\")\n",
    "    JSONfile.write(\"{'page':'\"+host+\n",
    "                   \"','scraping_date':'\"+datetime.today().strftime('%Y-%m-%d')+\n",
    "                   \"','hour':'\"+v_hours[i]+\":00\"+\n",
    "                   \"','temp':'\"+v_temps[i]+\n",
    "                   \"'}\\n\")  \n",
    "CSVfile.close()\n",
    "JSONfile.close()\n",
    "print(\"Data from \" + host + \" scraped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from pogoda.wp.pl scraped successfully\n"
     ]
    }
   ],
   "source": [
    "#Website pogoda.wp.pl (3)\n",
    "\n",
    "#Load website HTML\n",
    "page=\"https://pogoda.wp.pl/pogoda-na-dzis/gdansk/3099434\"\n",
    "r = requests.get(page)\n",
    "host = re.search('https://(.+?)/', page).group(1)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "#load values from HTML\n",
    "hours = soup.findAll('div',attrs = {'class':'data center','data-v-6afd4e19':\"\"})[:5]\n",
    "temps = soup.findAll('span',attrs = {'class':'temp','data-v-6afd4e19':\"\"})[1:6]\n",
    "\n",
    "#Write values to vectors\n",
    "v_hours=[]\n",
    "v_temps=[]\n",
    "for temp in temps:\n",
    "    v_temps.append(temp.text.strip('Â°C'))\n",
    "for hour in hours:\n",
    "    v_hours.append(hour.text.strip())\n",
    "    \n",
    "#Write data from vectors to files\n",
    "CSVfile = open('WeatherScraping.csv','a')\n",
    "JSONfile = open('WeatherScraping.json','a')\n",
    "for i in range(5):\n",
    "    CSVfile.write(\n",
    "        host+\",\"\n",
    "        +datetime.today().strftime('%Y-%m-%d')+\",\"\n",
    "        +v_hours[i]+\",\"\n",
    "        +v_temps[i]+\",\"\n",
    "        +\"NULL\"+\n",
    "        \"\\n\")\n",
    "    JSONfile.write(\"{'page':'\"+host+\n",
    "                   \"','scraping_date':'\"+datetime.today().strftime('%Y-%m-%d')+\n",
    "                   \"','hour':'\"+v_hours[i]+\n",
    "                   \"','temp':'\"+v_temps[i]+\n",
    "                   \"'}\\n\")  \n",
    "CSVfile.close()\n",
    "JSONfile.close()\n",
    "print(\"Data from \" + host + \" scraped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from pogoda.trojmiasto.pl scraped successfully\n"
     ]
    }
   ],
   "source": [
    "#Website pogoda.trojmiasto.pl (4)\n",
    "\n",
    "#Load website HTML\n",
    "page=\"https://pogoda.trojmiasto.pl/\"\n",
    "r = requests.get(page)\n",
    "host = re.search('https://(.+?)/', page).group(1)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "#load values from HTML\n",
    "hours = soup.findAll('div',attrs = {'class':'time'})[:5]\n",
    "temps = soup.findAll('div',attrs = {'class':'temp'})[:5]\n",
    "winds = soup.findAll('div',attrs = {'class':'wind_speed'})[:5]\n",
    "\n",
    "#Write values to vectors\n",
    "v_hours=[]\n",
    "v_temps=[]\n",
    "v_winds=[]\n",
    "for temp in temps:\n",
    "    v_temps.append(temp.text.strip('Â°C'))\n",
    "for hour in hours:\n",
    "    v_hours.append(hour.text.strip())\n",
    "for wind in winds:\n",
    "    v_winds.append(wind.text.strip('km/h'))    \n",
    "    \n",
    "#Write data from vectors to files\n",
    "CSVfile = open('WeatherScraping.csv','a')\n",
    "JSONfile = open('WeatherScraping.json','a')\n",
    "for i in range(5):\n",
    "    CSVfile.write(\n",
    "        host+\",\"\n",
    "        +datetime.today().strftime('%Y-%m-%d')+\",\"\n",
    "        +v_hours[i]+\",\"\n",
    "        +v_temps[i]+\",\"\n",
    "        +v_winds[i]+\n",
    "        \"\\n\")\n",
    "    JSONfile.write(\"{'page':'\"+host+\n",
    "                   \"','scraping_date':'\"+datetime.today().strftime('%Y-%m-%d')+\n",
    "                   \"','hour':'\"+v_hours[i]+\n",
    "                   \"','temp':'\"+v_temps[i]+\n",
    "                   \"','wind':'\"+v_winds[i]+\n",
    "                   \"'}\\n\")  \n",
    "CSVfile.close()\n",
    "JSONfile.close()\n",
    "print(\"Data from \" + host + \" scraped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose implemented function:\n",
      "1. Temperatures in selected scraping date\n",
      "2. Wind speeds in selected data crape\n",
      "3. Analysis of temperature in specific hour in specific date\n",
      "\n",
      "s\n",
      "Type existing number of function!\n",
      "5\n",
      "Type existing number of function!\n",
      "1\n",
      "Type date to analyse [yyyy-mm-dd]\n",
      "2021-01-24\n",
      "Average forecasted temperature in 2021-01-24 was 1.45 Â°C\n",
      "Min forecasted temperature in 2021-01-24 was 0 Â°C\n",
      "Max forecasted temperature in 2021-01-24 was 3 Â°C\n"
     ]
    }
   ],
   "source": [
    "#Data analysis:\n",
    "\n",
    "print(\"Choose implemented function:\\n\"\n",
    "    +\"1. Temperatures in selected scraping date\\n\"\n",
    "    +\"2. Wind speeds in selected data crape\\n\"\n",
    "    +\"3. Analysis of temperature in specific hour in specific date\\n\")\n",
    "while True:\n",
    "    try:\n",
    "        case = int(input())\n",
    "    except ValueError:\n",
    "        print (\"Type existing number of function!\")\n",
    "        continue\n",
    "    else:\n",
    "        if (case == 1) or (case == 2) or (case == 3):\n",
    "            break\n",
    "        else:\n",
    "            print(\"Type existing number of function!\")\n",
    "            \n",
    "with open('WeatherScraping.json','r') as JSON:\n",
    "    data=str(JSON.readlines())\n",
    "    Jdata=json.loads(data)\n",
    "    \n",
    "print(\"Type date to analyse [yyyy-mm-dd]\")\n",
    "day=input()\n",
    "vt=[]\n",
    "\n",
    "if case == 1:\n",
    "    for row in range (len(Jdata)):\n",
    "        search = eval(Jdata[row])\n",
    "        if 'temp' in search:\n",
    "            if (search.get('scraping_date') == day):\n",
    "                vt.append(int(search.get('temp')))\n",
    "    if (len(vt) == 0):\n",
    "        print(\"No data found\")\n",
    "    else:\n",
    "        print(\"Average forecasted temperature in\", day ,\"was\", statistics.mean(vt),\"Â°C\")\n",
    "        print(\"Min forecasted temperature in\", day ,\"was\", min(vt),\"Â°C\")\n",
    "        print(\"Max forecasted temperature in\", day ,\"was\", max(vt),\"Â°C\")\n",
    "elif case == 2:\n",
    "    for row in range (len(Jdata)):\n",
    "        search = eval(Jdata[row])\n",
    "        if 'wind' in search:\n",
    "            if (search.get('scraping_date') == day):\n",
    "                vt.append(int(search.get('wind')))\n",
    "    if (len(vt) == 0):\n",
    "        print(\"No data found\")\n",
    "    else:\n",
    "        print(\"Average wind speed\", day ,\"was\", statistics.mean(vt),\"km/h\")\n",
    "        print(\"Min forecasted wind speed in\", day ,\"was\", min(vt),\"km/h\")\n",
    "        print(\"Max forecasted wind speed in\", day ,\"was\", max(vt),\"km/h\")\n",
    "elif case == 3:\n",
    "    print(\"Type hour to analyse [hh]\")\n",
    "    hour = input()+\":00\"\n",
    "    for row in range (len(Jdata)):\n",
    "        search = eval(Jdata[row])\n",
    "        if 'temp' in search:\n",
    "            if (search.get('scraping_date') == day) and (search.get('hour') == hour):\n",
    "                vt.append(int(search.get('temp')))\n",
    "    if (len(vt) == 0):\n",
    "        print(\"No data found\")\n",
    "    else:\n",
    "        print(\"Average forecasted temperature in\", day ,\"and hour\", hour, \"was\", statistics.mean(vt),\"Â°C\")\n",
    "        print(\"Min forecasted temperature in\", day ,\"and hour\", hour, \"was\", min(vt),\"Â°C\")\n",
    "        print(\"Max forecasted temperature in\", day ,\"and hour\", hour, \"was\", max(vt),\"Â°C\")\n",
    "else:\n",
    "    print(\"No existing function\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
